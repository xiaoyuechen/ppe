\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{booktabs}

\title{Lab 3}
\author{Hannah Atmer and Xiaoyue Chen \\ Team 6}
\date{October 2021}

\begin{document}

\maketitle

\section{Performance Estimates}
  \begin{table}[h]
    \centering
    \begin{tabular}{p{0.3\textwidth}p{0.3\textwidth}p{0.4\textwidth}}
      \toprule
      Operation & Predicted performance & Why \\
      \midrule
      8-bit unsigned add & 787 GOPS & \(6 \times 4.1\times10^{9} \times 32 \times 1\) \\
      floating point add & 196 GFLOPS & \(6 \times 4.1\times10^{9} \times 8 \times 1\) \\
      Sequentially load a 32-bit int from an array & 3.6 GOPS & \(14.4 / 4\) \\
      Randomly load a 32 bit int from an array & 0.2 GOPS & \(14.4 / 16 / 4\) \\
      \bottomrule
    \end{tabular}
    \caption{Performance estimation for CPU}
  \end{table}

  \begin{table}[h]
    \centering
    \begin{tabular}{p{0.3\textwidth}p{0.3\textwidth}p{0.4\textwidth}}
      \toprule
      Operation & Predicted performance & Why \\
      \midrule
      8-bit unsigned add & 1734 GOPS & \(1280 \times 1.7 \times 1\) \\
      floating point add & 1734 GFLOPS & \(1280 \times 1.7 \times 1\) \\
      Sequentially load a 32-bit int from an array & 48 GOPS & \(192 / 4\) \\
      Randomly load a 32 bit int from an array & 1.5 GOPS & \(192 / 32
                                                            / 4\) \\
      Data transfer from CPU to GPU & 1.5 GB/s & measured \\
      \bottomrule
    \end{tabular}
    \caption{Performance estimation for GPU}
  \end{table}

\section{Performance Achieved}

\subsection{Your measured performance results from implementing the optimizations.}

\subsection{The parallel scaling of your implementation.}

\section{Discussion}

\subsection{A discussion of the measured results and how they differ
  from your predictions.}
We expected the kernel execution would be 50 times faster than the CPU
version. The measured results show that the actual speedup is around
14 times. The overhead might come from calling OpenCL api calls.

We expected the data movement would be the major bottleneck. The
measured results show that it is indeed the case. The data movement
time is even longer than we expected. Around 300 ms is spent on
data movement while only 7 ms was spent on the kernel execution.

\subsection{A discussion of what you learned about these optimizations from implementing them and measuring the results.}
We learned that copying the data to and from the GPU took more time than running the code without a GPU. We also learned that it is hard to debug GPU kernel code since the kernel code is compiled at runtime.

\subsection{Comment on any unexpected or odd results.}
Copying data between the CPU and GPU took 300ms total. 200 of these ms were spent copying data from the GPU, meaning that it was half as fast to copy data back from the GPU.


\subsection{Comments on the difficulty of the GPU offload.}
We used the in-class example, and this worked well except for the lack of informative compiler messages about errors in the kernel code.

\section{Lab comments}
We ended up doing a lot of code improvements aside from optimizations. Perhaps our improved code can be used in later years.

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
